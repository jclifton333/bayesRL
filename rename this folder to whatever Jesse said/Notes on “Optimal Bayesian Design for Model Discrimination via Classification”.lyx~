#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
Notes on 
\begin_inset Quotes eld
\end_inset

Optimal Bayesian Design for Model Discrimination via Classification
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
Setup
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $K$
\end_inset

 candidate statistical models, one of them is true
\end_layout

\begin_layout Itemize
Models indexed by 
\begin_inset Formula $M$
\end_inset

, a random variable in 
\begin_inset Formula $\{1,\dots,K\}$
\end_inset


\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $M=m$
\end_inset

, model is 
\begin_inset Formula $p(y|\theta_{m},m,d)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $y\in{\cal Y}$
\end_inset

 is data vector
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta_{m}\in\Theta_{m}$
\end_inset

 is parameter vector
\end_layout

\begin_layout Itemize
\begin_inset Formula $d\in{\cal D}$
\end_inset

 is design vector, the variables governing the experiment
\end_layout

\end_deeper
\begin_layout Itemize
Prior on 
\begin_inset Formula $\theta_{m}$
\end_inset

 is 
\begin_inset Formula $p(\theta_{m}|m)$
\end_inset

, given for each possible model
\end_layout

\begin_layout Itemize
Prior on 
\begin_inset Formula $m$
\end_inset

 is 
\begin_inset Formula $p(m)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Picking 
\begin_inset Formula $d$
\end_inset

.
 Can use loss function that might depend on 
\begin_inset Formula $m$
\end_inset

, 
\begin_inset Formula $\theta_{m}$
\end_inset

, and 
\begin_inset Formula $y$
\end_inset

, write it as 
\begin_inset Formula 
\[
l(d)=E_{\theta_{m},y,M|d}[l(d,\theta_{m},y,M)].
\]

\end_inset

Optimal 
\begin_inset Formula $d$
\end_inset

 is 
\begin_inset Formula $d^{*}=\arg\min_{d\in{\cal D}}l(d)$
\end_inset

.
 We want to use an 
\begin_inset Formula $l$
\end_inset

 that captures entropy around the distribution of 
\begin_inset Formula $M$
\end_inset

 (which we then minimize).
 So we want to pick a 
\begin_inset Formula $d$
\end_inset

, which produces data 
\begin_inset Formula $y$
\end_inset

, that leads to the most information about the distribution of 
\begin_inset Formula $M$
\end_inset

.
 This will help us pick the correct model.
\end_layout

\begin_layout Itemize
Choice of loss function.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Shannon entropy (
\begin_inset Formula $MD$
\end_inset

 stands for multinomial deviance).
\begin_inset Formula 
\[
l_{MD}:\begin{cases}
{\cal D}\times{\cal Y}\to[0,\infty)\\
(d,y)\mapsto-\sum_{m=1}^{K}p(m|y,d)\log p(m|y,d)
\end{cases}.
\]

\end_inset


\begin_inset Formula $y$
\end_inset

 is observed after the experiment, so we don't actually know 
\begin_inset Formula $l_{MD}$
\end_inset

.
 So we'll integrate it out: they rewrite 
\begin_inset Formula $l_{MD}$
\end_inset

 as 
\begin_inset Formula 
\begin{eqnarray}
l_{MD}(d) & = & -\int_{y}p(y|d)\sum_{m=1}^{K}p(m|y,d)\log p(m|y,d)dy\nonumber \\
 & \stackrel{\text{Bayes' Rule}}{=} & -\sum_{m=1}^{K}p(m)\int_{y}p(y|m,d)\log p(m|y,d)dy.\label{eq:2.1}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Itemize
Misclassification error rate.
 Use a loss matrix for all combinations of true and selected models:
\begin_inset Formula 
\begin{align}
l_{01}(d) & =\int_{y}p(y|d)\sum_{m=1}^{K}p(m|y,d)\{1-\text{I}[\widehat{m}(y|d)=m]\}dy\nonumber \\
 & =\int_{y}p(y|d)\{1-p[\widehat{m}(y|d)|y,d]\}dy,\label{eq:2.2}
\end{align}

\end_inset

where 
\begin_inset Formula $\widehat{m}:{\cal Y}\to\{1,\dots,K\}$
\end_inset

 is a classifier.
\end_layout

\begin_layout Standard
The loss functions above can be hard to compute analytically, so we can
 use Monte Carlo integration, by sampling from 
\begin_inset Formula $p(m|y,d)$
\end_inset

.
 Some common issues:
\end_layout

\begin_layout Itemize
Need to draw a lot of samples from 
\begin_inset Formula $p(m|y,d)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $p(m|y,d)$
\end_inset

 is not always available and needs to be estimated
\end_layout

\begin_layout Itemize
The likelihood 
\begin_inset Formula $p(y|\theta_{m},m,d)$
\end_inset

 is intractable
\end_layout

\begin_layout Itemize
Need a lot of data/Monte Carlo samples for reasonable accuracy
\end_layout

\begin_layout Standard
Some remedies:
\end_layout

\begin_layout Itemize
using conjugate priors so integrals can be computed analytically
\end_layout

\begin_layout Itemize
quadrature
\end_layout

\begin_layout Itemize
sequential Monte Carlo (only applicable to sequential experimental designs,
 where design space is small)
\end_layout

\begin_layout Itemize
Gaussian-based posterior approximation
\end_layout

\begin_layout Itemize
ABC
\end_layout

\begin_layout Standard
Some shortcomings of ABC are addressed by classification method.
\end_layout

\end_deeper
\begin_layout Itemize
Classification approach.
 Consider the Monte Carlo integral of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:2.2"
plural "false"
caps "false"
noprefix "false"

\end_inset

: 
\begin_inset Formula 
\begin{equation}
\widehat{l}_{01}(d)=1-\sum_{m=1}^{K}p(m)\frac{1}{J}\sum_{j=1}^{J}\text{I}[\widehat{m}(y^{m,j}|d)=m],\label{eq:2.4}
\end{equation}

\end_inset

where 
\begin_inset Formula $y^{m,j}\sim p(y|m,d)$
\end_inset

 for 
\begin_inset Formula $j\in\{1,\dots,J\}$
\end_inset

, 
\begin_inset Formula $m\in\{1,\dots,K\}$
\end_inset

, and 
\begin_inset Formula $\widehat{m}$
\end_inset

 is the Bayes classifier, which depends on 
\begin_inset Formula $p(m|y,d)$
\end_inset

.
 As before, this has a few issues:
\end_layout

\begin_deeper
\begin_layout Itemize
Optimizing this loss function, we may need a high number of replicates 
\begin_inset Formula $J$
\end_inset

 to estimate 
\begin_inset Formula $l$
\end_inset

 with low variance.
\end_layout

\begin_layout Itemize
\begin_inset Formula $p(y|m,d)$
\end_inset

 may need to be estimated before sampling 
\begin_inset Formula $y^{m,j}$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $p(m|y,d)$
\end_inset

 is hard to estimate when the likelihood 
\begin_inset Formula $p(y|\theta_{m},m,d)$
\end_inset

 is difficult to compute.
\end_layout

\begin_layout Standard
Consider the following scheme: consider a sample 
\begin_inset Formula ${\cal T}=\{m^{j},y^{j}\}_{n=1}^{J}$
\end_inset

 from 
\begin_inset Formula 
\[
p(y,m|d)=\int_{\theta_{m}}p(y|\theta_{m},m,d)p(\theta_{m}|m)p(m)d\theta_{m},
\]

\end_inset

then using 
\begin_inset Formula ${\cal T}$
\end_inset

 to train a classifier 
\begin_inset Formula $\widehat{m}_{C}:{\cal Y}\to\{1,\dots,K\}$
\end_inset

 using 
\begin_inset Formula ${\cal T}$
\end_inset

.
 Using this classifier, and new data 
\begin_inset Formula ${\cal T}_{*}=\{m_{*}^{j},y_{*}^{j}\}_{j=1}^{J_{*}}$
\end_inset

, the analogue of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:2.4"
plural "false"
caps "false"
noprefix "false"

\end_inset

 would be 
\begin_inset Formula 
\[
\widehat{l}_{01}(d)=1-\frac{1}{J_{*}}\sum_{j=1}^{J_{*}}I[\widehat{m}_{C}(y_{*}^{j}|d,{\cal T})=m_{*}^{j}].
\]

\end_inset

For intractable likelihoods, the number of samples 
\begin_inset Formula $J$
\end_inset

 and 
\begin_inset Formula $J^{*}$
\end_inset

 to approximate 
\begin_inset Formula $l_{01}$
\end_inset

 are smaller than required by ABC.
 This scheme just requires being able to efficiently simulate from the 
\begin_inset Formula $K$
\end_inset

 models.
 The misclassification error rate may not be well estimated, but we just
 require that the designs are ranked correctly according to 
\begin_inset Formula $\hat{l}_{01}$
\end_inset

.
 Classification methods can also be adapted to 
\begin_inset Formula $l_{MD}$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:2.1"
plural "false"
caps "false"
noprefix "false"

\end_inset

, but estimates of the posterior over models can be poor.
 But we'll just focus on constructing a classifier that is order-correct,
 that is it assigns 
\begin_inset Formula $\arg\max_{m}p(m|y,d)$
\end_inset

 for each 
\begin_inset Formula $y\in{\cal Y}$
\end_inset

.
 That way, the misclassification rate for a classifier that is order correct
 except on a small subset of 
\begin_inset Formula ${\cal Y}$
\end_inset

 will be still be close to the Bayes error rate.
\end_layout

\begin_layout Itemize
We use CART and not random forests since their posterior estimates 
\begin_inset Formula $\widehat{p}$
\end_inset

 may be 0 and thus the multinomial deviance would be 
\begin_inset Formula $\infty$
\end_inset

.
\end_layout

\begin_layout Itemize
Since the loss function is evaluated many times, may need to trees just
 trained on training set, which may lead to over-fitting
\end_layout

\begin_deeper
\begin_layout Itemize
Again we just need the ranking of the designs to be correct
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Examples
\end_layout

\begin_layout Itemize

\end_layout

\end_body
\end_document
